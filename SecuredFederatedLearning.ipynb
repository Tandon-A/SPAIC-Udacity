{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SecuredFederatedLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tandon-A/SPAIC-Udacity/blob/master/SecuredFederatedLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUQ5Cfydp7pZ",
        "colab_type": "text"
      },
      "source": [
        "**Secured Fedrated Learning**\n",
        "\n",
        "Federated Learning allows for a machine learning model to train on user device's where the data is present. This allows to train models directly on the data without ever bringing the data to the company server. \n",
        "Once training is completed the models are brought upto the server for aggregation. \n",
        "\n",
        "As the model has learnt on user data, it holds information about the user data which can be used malicious users by using techniques such as checking the activation maps of the model for different set of inputs.\n",
        "\n",
        "In order to prevent this, in secured federated learning, the aggregation of all the trained models is done by a trusted party and then this aggregated model is sent to the company server. \n",
        "\n",
        "In this example, the aggregation is performed by another virtual worker - 'arya'. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMIdjMbioduo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install syft "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beKrSjpnoh-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "bf69c710-984c-4346-8506-839472a3868f"
      },
      "source": [
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from collections import OrderedDict\n",
        "import torch \n",
        "\n",
        "#Loading the MNIST data\n",
        "mnist = datasets.MNIST(root='./data', train=True, download=True, transform= transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "trainX = mnist.train_data\n",
        "trainY = mnist.train_labels\n",
        "np.random.seed(7)\n",
        "#Shuffling MNIST data\n",
        "perm = np.random.permutation(len(trainX))\n",
        "trainX = trainX[perm,:]\n",
        "trainY = trainY[perm]\n",
        "#Preparing training data for two workers\n",
        "val_x = trainX[40000:50000]\n",
        "val_y = trainY[40000:50000]\n",
        "tr_x1 = trainX[0:5000]\n",
        "tr_y1 = trainY[0:5000]\n",
        "tr_x2 = trainX[10000:15000]\n",
        "tr_y2 = trainY[10000:15000]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8452691.77it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 126355.20it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2062542.87it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 47771.82it/s]            \n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4Z78jDHpCGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to create a NN model\n",
        "def create_model():\n",
        "  return nn.Sequential(OrderedDict([\n",
        "    ('fc1',nn.Linear(784,512)),\n",
        "    ('r1',nn.ReLU()),\n",
        "    ('fc2',nn.Linear(512,256)),\n",
        "    ('r2',nn.ReLU()),\n",
        "    ('fc3',nn.Linear(256,64)),\n",
        "    ('r3',nn.ReLU()),\n",
        "    ('fc4',nn.Linear(64,10)),\n",
        "    ('logps',nn.LogSoftmax(dim=1))\n",
        "]))\n",
        "\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sQ1mUKepH8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "8b99fab3-55c1-4b00-9e96-659f770d1e1d"
      },
      "source": [
        "import syft as sy \n",
        "\n",
        "#Creating a hook between PySyft and Pytorch\n",
        "hook = sy.TorchHook(torch)\n",
        "#Creating three virtual workers\n",
        "bob = sy.VirtualWorker(hook,\"bob\")\n",
        "alice = sy.VirtualWorker(hook,\"alice\")\n",
        "arya = sy.VirtualWorker(hook,\"arya\")\n",
        "\n",
        "bob.clear_objects()\n",
        "alice.clear_objects()\n",
        "arya.clear_objects()\n",
        "print (bob._objects,alice._objects,arya._objects)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{} {} {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHeqGPQTvvHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4f3f5fd1-6cdd-4f8c-b7cd-7bec7378389a"
      },
      "source": [
        "datasets = [(tr_x1,tr_y1),(tr_x2,tr_y2)]\n",
        "models = list()\n",
        "workers = [bob,alice]\n",
        "\n",
        "for i in range(len(workers)):\n",
        "  #sending validation data to specific worker\n",
        "  val_x_worker = val_x.clone().send(workers[i])\n",
        "  val_y_worker = val_y.clone().send(workers[i])\n",
        "  tr_x,tr_y = datasets[i]\n",
        "  #sending dataset to worker\n",
        "  tr_x = tr_x.send(workers[i])\n",
        "  tr_y = tr_y.send(workers[i])\n",
        "  #creating a model for a worker\n",
        "  model_worker = create_model()\n",
        "  #creating optimizer for worker -- right now pysyft only allows for SGD as optimizer\n",
        "  opt = optim.SGD(model_worker.parameters(),lr=0.001)\n",
        "  model_worker.send(workers[i])\n",
        "  \n",
        "  \n",
        "  #training worker models for 5 epochs \n",
        "  epochs = 5\n",
        "  batch_size = 50\n",
        "  batches = len(tr_x)//batch_size\n",
        "  #setting worker model in train mode\n",
        "  model_worker.train()\n",
        "  print (\"starting training for model \",i)\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for batch in range(batches):\n",
        "      opt.zero_grad()\n",
        "      images = tr_x[batch*batch_size : (batch+1)*batch_size].float().view(batch_size,-1)\n",
        "      labels = tr_y[batch*batch_size : (batch+1)*batch_size]\n",
        "      log_ps = model_worker(images)\n",
        "      loss = criterion(log_ps,labels)\n",
        "      running_loss += loss\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "    running_loss = running_loss.get()\n",
        "    #print (running_loss)\n",
        "  #testing worker model on validation data  \n",
        "  with torch.no_grad():\n",
        "    model_worker.eval()\n",
        "    ps = torch.exp(model_worker(val_x_worker.view(val_x_worker.shape[0],-1).float()))\n",
        "    topp,topk = ps.topk(1,dim=1)\n",
        "    equals = topk == val_y_worker.view(*topk.shape)\n",
        "    equals = equals.get()\n",
        "    accuracy = torch.mean(equals.type(torch.FloatTensor)) \n",
        "    print (\"worker model = %r accuracy = %r \" %(i,accuracy))\n",
        "  #saving the worker model  \n",
        "  models.append(model_worker)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting training for model  0\n",
            "worker model = 0 accuracy = tensor(0.9024) \n",
            "starting training for model  1\n",
            "worker model = 1 accuracy = tensor(0.9015) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWv0WggHpaZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_alice = models[-1]\n",
        "model_bob = models[-2]\n",
        "\n",
        "print (model_alice.location,model_bob.location)\n",
        "#Sending the models to arya worker\n",
        "model_bob.move(arya)\n",
        "model_alice.move(arya)\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "#aggregating the models on arya worker device \n",
        "with torch.no_grad():\n",
        "  for p1,p2,p3 in zip(model.parameters(),model_bob.parameters(),model_alice.parameters()):\n",
        "    p1.set_((p2+p3).get()/2)  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}