{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FederatedLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tandon-A/SPAIC-Udacity/blob/master/FederatedLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXStHuwOtMYB",
        "colab_type": "text"
      },
      "source": [
        "**Federated Learning**\n",
        "\n",
        "Federated Learning allows for a machine learning model to train on user device's where the data is present. This allows to train models directly on the data without ever bringing the data to the company server. Once training is completed the models are brought upto the server for aggregation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx5J2pyHfwQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install syft "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GiBmSwsk5x7",
        "colab_type": "code",
        "outputId": "8bb808df-a268-4201-cf75-fb7d0d36e421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from collections import OrderedDict\n",
        "import torch \n",
        "\n",
        "\n",
        "#Loading MNIST data \n",
        "mnist = datasets.MNIST(root='./data', train=True, download=True, transform= transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "trainX = mnist.train_data\n",
        "trainY = mnist.train_labels\n",
        "np.random.seed(7) \n",
        "\n",
        "#shuffling MNIST data\n",
        "perm = np.random.permutation(len(trainX))\n",
        "trainX = trainX[perm,:]\n",
        "trainY = trainY[perm]\n",
        "\n",
        "#Dividing the dataset into three training sets and a validation set \n",
        "val_x = trainX[40000:50000]\n",
        "val_y = trainY[40000:50000]\n",
        "tr_x1 = trainX[0:10000]\n",
        "tr_y1 = trainY[0:10000]\n",
        "tr_x2 = trainX[10000:20000]\n",
        "tr_y2 = trainY[10000:20000]\n",
        "tr_x3 = trainX[20000:30000]\n",
        "tr_y3 = trainY[20000:30000]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:03, 2673412.76it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 56802.39it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 936184.41it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21410.27it/s]            \n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXbxQmmbmMlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function for creating a pytorch model\n",
        "def create_model():\n",
        "  return nn.Sequential(OrderedDict([\n",
        "    ('fc1',nn.Linear(784,512)),\n",
        "    ('r1',nn.ReLU()),\n",
        "    ('dropout1',nn.Dropout(p=0.2)),\n",
        "    ('fc2',nn.Linear(512,512)),\n",
        "    ('r2',nn.ReLU()),\n",
        "    ('dropout2',nn.Dropout(p=0.2)),\n",
        "    ('fc3',nn.Linear(512,256)),\n",
        "    ('r3',nn.ReLU()), \n",
        "    ('fc4',nn.Linear(256,64)),\n",
        "    ('r4',nn.ReLU()),\n",
        "    ('fc5',nn.Linear(64,10)),\n",
        "    ('log_ps',nn.LogSoftmax(dim=1))]))\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzw8M4_XmXa1",
        "colab_type": "code",
        "outputId": "e7b7e890-a730-43b8-b2d3-5bfd512b2782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import syft as sy \n",
        "\n",
        "#Creating a torch hook for PySyft\n",
        "hook = sy.TorchHook(torch)\n",
        "#Creating three virtual workers for federated learning \n",
        "bob = sy.VirtualWorker(hook,id='bob')\n",
        "alice = sy.VirtualWorker(hook,id='alice')\n",
        "arya = sy.VirtualWorker(hook,id='arya')\n",
        "\n",
        "bob.clear_objects()\n",
        "alice.clear_objects()\n",
        "arya.clear_objects()\n",
        "print (bob._objects,alice._objects,arya._objects)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{} {} {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjeJmU9dm2y4",
        "colab_type": "code",
        "outputId": "cfad7dc6-8adb-4717-b117-e4a09d4e1eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "workers = [bob,alice,arya]\n",
        "datasets = [(tr_x1,tr_y1),(tr_x2,tr_y2),(tr_x3,tr_y3)]\n",
        "#list for saving the models trained on different workers\n",
        "models = list()\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(workers)):\n",
        "  #sending validation data to specific worker\n",
        "  val_x_worker = val_x.clone().send(workers[i])\n",
        "  val_y_worker = val_y.clone().send(workers[i])\n",
        "  tr_x,tr_y = datasets[i]\n",
        "  #sending dataset to worker\n",
        "  tr_x = tr_x.send(workers[i])\n",
        "  tr_y = tr_y.send(workers[i])\n",
        "  #creating a model for a worker\n",
        "  model_worker = create_model()\n",
        "  #creating optimizer for worker -- right now pysyft only allows for SGD as optimizer\n",
        "  opt = optim.SGD(model_worker.parameters(),lr=0.001)\n",
        "  model_worker.send(workers[i])\n",
        "  \n",
        "  \n",
        "  #training worker models for 5 epochs \n",
        "  epochs = 5\n",
        "  batch_size = 50\n",
        "  batches = len(tr_x)//batch_size\n",
        "  #setting worker model in train mode\n",
        "  model_worker.train()\n",
        "  print (\"starting training for model \",i)\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for batch in range(batches):\n",
        "      opt.zero_grad()\n",
        "      images = tr_x[batch*batch_size : (batch+1)*batch_size].float().view(batch_size,-1)\n",
        "      labels = tr_y[batch*batch_size : (batch+1)*batch_size]\n",
        "      log_ps = model_worker(images)\n",
        "      loss = criterion(log_ps,labels)\n",
        "      running_loss += loss\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "    running_loss = running_loss.get()\n",
        "    #print (running_loss)\n",
        "  #testing worker model on validation data  \n",
        "  with torch.no_grad():\n",
        "    model_worker.eval()\n",
        "    ps = torch.exp(model_worker(val_x_worker.view(val_x_worker.shape[0],-1).float()))\n",
        "    topp,topk = ps.topk(1,dim=1)\n",
        "    equals = topk == val_y_worker.view(*topk.shape)\n",
        "    equals = equals.get()\n",
        "    accuracy = torch.mean(equals.type(torch.FloatTensor)) \n",
        "    print (\"worker model = %r accuracy = %r \" %(i,accuracy))\n",
        "  #saving the worker model  \n",
        "  models.append(model_worker)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting training for model  0\n",
            "worker model = 0 accuracy = tensor(0.9145) \n",
            "starting training for model  1\n",
            "worker model = 1 accuracy = tensor(0.9154) \n",
            "starting training for model  2\n",
            "worker model = 2 accuracy = tensor(0.9141) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjwwohMYqNQH",
        "colab_type": "code",
        "outputId": "fe99f4fd-a887-4632-c616-f67fea4497c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model_arya = models[-1]\n",
        "model_alice = models[-2]\n",
        "model_bob = models[-3]\n",
        "print (model_arya.location,model_alice.location,model_bob.location)\n",
        "#shifting all models to bob worker for aggregation\n",
        "model_arya.move(bob)\n",
        "model_alice.move(bob)\n",
        "print (model_arya.location,model_alice.location,model_bob.location)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<VirtualWorker id:arya #objects:26> <VirtualWorker id:alice #objects:15> <VirtualWorker id:bob #objects:15>\n",
            "<VirtualWorker id:bob #objects:35> <VirtualWorker id:bob #objects:35> <VirtualWorker id:bob #objects:35>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-doHPFjNqn7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_model = create_model().send(bob)\n",
        "bobp = model_bob.parameters()\n",
        "alicep = model_alice.parameters()\n",
        "aryap = model_arya.parameters()\n",
        "finalp = final_model.parameters()\n",
        "\n",
        "#the final model now stores the aggregation of all the trained models \n",
        "with torch.no_grad():\n",
        "  for pf,pb,pa,par in zip(finalp,bobp,alicep,aryap):\n",
        "    pf.set_((pb + pa + par)/3)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}